{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vIrtxsTJ5Ff5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import cv2\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import time\n",
    "from sys import maxsize\n",
    "import SimpleITK as sitk\n",
    "import wandb\n",
    "import optuna\n",
    "import nibabel as nib\n",
    "from torchmetrics import JaccardIndex\n",
    "from torchvision.transforms import CenterCrop\n",
    "import wandb\n",
    "# import torch.multiprocessing as mp\n",
    "# import torch.distributed as dist\n",
    "# from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "# from torch.utils.data.distributed import DistributedSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mviraj-patil0911\u001b[0m (\u001b[33mbrats_2024_3d_unet\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "RV45t1xmQhnB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# prompt: cuda or cpu check\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/data/mpstme-priyanka/training_data1_v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_crop(tensor, target_dims):\n",
    "    \n",
    "        current_dims= tensor.shape\n",
    "        start_indices = [(curr_dim - target_dim) // 2 for curr_dim, target_dim in zip(current_dims, target_dims)]\n",
    "        end_indices = [start + target_dim for start, target_dim in zip(start_indices, target_dims)]\n",
    "\n",
    "        cropped_data = tensor[\n",
    "            start_indices[0]:end_indices[0],\n",
    "            start_indices[1]:end_indices[1],\n",
    "            start_indices[2]:end_indices[2],\n",
    "\n",
    "        ]\n",
    "\n",
    "        return cropped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8DWk8FVJLi4v",
    "outputId": "1a2d6967-1968-41bc-fb04-ae9c9933e1ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total samples 1\n",
      "total samples 2\n",
      "total samples 3\n",
      "total samples 4\n",
      "total samples 5\n",
      "total samples 6\n",
      "total samples 7\n",
      "total samples 8\n",
      "total samples 9\n",
      "total samples 10\n",
      "total samples 11\n",
      "total samples 12\n",
      "total samples 13\n",
      "total samples 14\n",
      "total samples 15\n",
      "total samples 16\n",
      "total samples 17\n",
      "total samples 18\n",
      "total samples 19\n",
      "total samples 20\n",
      "total samples 21\n",
      "total samples 22\n",
      "total samples 23\n",
      "total samples 24\n",
      "total samples 25\n",
      "total samples 26\n",
      "total samples 27\n",
      "total samples 28\n",
      "total samples 29\n",
      "total samples 30\n",
      "total samples 31\n",
      "total samples 32\n",
      "total samples 33\n",
      "total samples 34\n",
      "total samples 35\n",
      "total samples 36\n",
      "total samples 37\n",
      "total samples 38\n",
      "total samples 39\n",
      "total samples 40\n",
      "total samples 41\n",
      "total samples 42\n",
      "total samples 43\n",
      "total samples 44\n",
      "total samples 45\n",
      "total samples 46\n",
      "total samples 47\n",
      "total samples 48\n",
      "total samples 49\n",
      "total samples 50\n",
      "total samples 51\n",
      "total samples 52\n",
      "total samples 53\n",
      "total samples 54\n",
      "total samples 55\n",
      "total samples 56\n",
      "total samples 57\n",
      "total samples 58\n",
      "total samples 59\n",
      "total samples 60\n",
      "total samples 61\n",
      "total samples 62\n",
      "total samples 63\n",
      "total samples 64\n",
      "total samples 65\n",
      "total samples 66\n",
      "total samples 67\n",
      "total samples 68\n",
      "total samples 69\n",
      "total samples 70\n",
      "total samples 71\n",
      "total samples 72\n",
      "total samples 73\n",
      "total samples 74\n",
      "total samples 75\n",
      "total samples 76\n",
      "total samples 77\n",
      "total samples 78\n",
      "total samples 79\n",
      "total samples 80\n",
      "total samples 81\n",
      "total samples 82\n",
      "total samples 83\n",
      "total samples 84\n",
      "total samples 85\n",
      "total samples 86\n",
      "total samples 87\n",
      "total samples 88\n",
      "total samples 89\n",
      "total samples 90\n",
      "total samples 91\n",
      "total samples 92\n",
      "total samples 93\n",
      "total samples 94\n",
      "total samples 95\n",
      "total samples 96\n",
      "total samples 97\n",
      "total samples 98\n",
      "total samples 99\n",
      "total samples 100\n",
      "total samples 101\n",
      "total samples 102\n",
      "total samples 103\n",
      "total samples 104\n",
      "total samples 105\n",
      "total samples 106\n",
      "total samples 107\n",
      "total samples 108\n",
      "total samples 109\n",
      "total samples 110\n",
      "total samples 111\n",
      "total samples 112\n",
      "total samples 113\n",
      "total samples 114\n",
      "total samples 115\n",
      "total samples 116\n",
      "total samples 117\n",
      "total samples 118\n",
      "total samples 119\n",
      "total samples 120\n",
      "total samples 121\n",
      "total samples 122\n",
      "total samples 123\n",
      "total samples 124\n",
      "total samples 125\n",
      "total samples 126\n",
      "total samples 127\n",
      "total samples 128\n",
      "total samples 129\n",
      "total samples 130\n",
      "total samples 131\n",
      "total samples 132\n",
      "total samples 133\n",
      "total samples 134\n",
      "total samples 135\n",
      "total samples 136\n",
      "total samples 137\n",
      "total samples 138\n",
      "total samples 139\n",
      "total samples 140\n",
      "total samples 141\n",
      "total samples 142\n",
      "total samples 143\n",
      "total samples 144\n",
      "total samples 145\n",
      "total samples 146\n",
      "total samples 147\n",
      "total samples 148\n",
      "total samples 149\n",
      "total samples 150\n",
      "total samples 151\n",
      "total samples 152\n",
      "total samples 153\n",
      "total samples 154\n",
      "total samples 155\n",
      "total samples 156\n",
      "total samples 157\n",
      "total samples 158\n",
      "total samples 159\n",
      "total samples 160\n",
      "total samples 161\n",
      "total samples 162\n",
      "total samples 163\n",
      "total samples 164\n",
      "total samples 165\n",
      "total samples 166\n",
      "total samples 167\n",
      "total samples 168\n",
      "total samples 169\n",
      "total samples 170\n",
      "total samples 171\n",
      "total samples 172\n",
      "total samples 173\n",
      "total samples 174\n",
      "total samples 175\n",
      "total samples 176\n",
      "total samples 177\n",
      "total samples 178\n",
      "total samples 179\n",
      "total samples 180\n",
      "total samples 181\n",
      "total samples 182\n",
      "total samples 183\n",
      "total samples 184\n",
      "total samples 185\n",
      "total samples 186\n",
      "total samples 187\n",
      "total samples 188\n",
      "total samples 189\n",
      "total samples 190\n",
      "total samples 191\n",
      "total samples 192\n",
      "total samples 193\n",
      "total samples 194\n",
      "total samples 195\n",
      "total samples 196\n",
      "total samples 197\n",
      "total samples 198\n",
      "total samples 199\n",
      "total samples 200\n",
      "total samples 201\n",
      "total samples 202\n",
      "total samples 203\n",
      "total samples 204\n",
      "total samples 205\n",
      "total samples 206\n",
      "total samples 207\n",
      "total samples 208\n",
      "total samples 209\n",
      "total samples 210\n",
      "total samples 211\n",
      "total samples 212\n",
      "total samples 213\n",
      "total samples 214\n",
      "total samples 215\n",
      "total samples 216\n",
      "total samples 217\n",
      "total samples 218\n",
      "total samples 219\n",
      "total samples 220\n",
      "total samples 221\n",
      "total samples 222\n",
      "total samples 223\n",
      "total samples 224\n",
      "total samples 225\n",
      "total samples 226\n",
      "total samples 227\n",
      "total samples 228\n",
      "total samples 229\n",
      "total samples 230\n",
      "total samples 231\n",
      "total samples 232\n",
      "total samples 233\n",
      "total samples 234\n",
      "total samples 235\n",
      "total samples 236\n",
      "total samples 237\n",
      "total samples 238\n",
      "total samples 239\n",
      "total samples 240\n",
      "total samples 241\n",
      "total samples 242\n",
      "total samples 243\n",
      "total samples 244\n",
      "total samples 245\n",
      "total samples 246\n",
      "total samples 247\n",
      "total samples 248\n",
      "total samples 249\n",
      "total samples 250\n",
      "total samples 251\n",
      "total samples 252\n",
      "total samples 253\n",
      "total samples 254\n",
      "total samples 255\n",
      "total samples 256\n",
      "total samples 257\n",
      "total samples 258\n",
      "total samples 259\n",
      "total samples 260\n",
      "total samples 261\n",
      "total samples 262\n",
      "total samples 263\n",
      "total samples 264\n",
      "total samples 265\n",
      "total samples 266\n",
      "total samples 267\n",
      "total samples 268\n",
      "total samples 269\n",
      "total samples 270\n",
      "total samples 271\n",
      "total samples 272\n",
      "total samples 273\n",
      "total samples 274\n",
      "total samples 275\n",
      "total samples 276\n",
      "total samples 277\n",
      "total samples 278\n",
      "total samples 279\n",
      "total samples 280\n",
      "total samples 281\n",
      "total samples 282\n",
      "total samples 283\n",
      "total samples 284\n",
      "total samples 285\n",
      "total samples 286\n",
      "total samples 287\n",
      "total samples 288\n",
      "total samples 289\n",
      "total samples 290\n",
      "total samples 291\n",
      "total samples 292\n",
      "total samples 293\n",
      "total samples 294\n",
      "total samples 295\n",
      "total samples 296\n",
      "total samples 297\n",
      "total samples 298\n",
      "total samples 299\n",
      "total samples 300\n",
      "total samples 301\n",
      "total samples 302\n",
      "total samples 303\n",
      "total samples 304\n",
      "total samples 305\n",
      "total samples 306\n",
      "total samples 307\n",
      "total samples 308\n",
      "total samples 309\n",
      "total samples 310\n",
      "total samples 311\n",
      "total samples 312\n",
      "total samples 313\n",
      "total samples 314\n",
      "total samples 315\n",
      "total samples 316\n",
      "total samples 317\n",
      "total samples 318\n",
      "total samples 319\n",
      "total samples 320\n",
      "total samples 321\n",
      "total samples 322\n",
      "total samples 323\n",
      "total samples 324\n",
      "total samples 325\n",
      "total samples 326\n",
      "total samples 327\n",
      "total samples 328\n",
      "total samples 329\n",
      "total samples 330\n",
      "total samples 331\n",
      "total samples 332\n",
      "total samples 333\n",
      "total samples 334\n",
      "total samples 335\n",
      "total samples 336\n",
      "total samples 337\n",
      "total samples 338\n",
      "total samples 339\n",
      "total samples 340\n",
      "total samples 341\n",
      "total samples 342\n",
      "total samples 343\n",
      "total samples 344\n",
      "total samples 345\n",
      "total samples 346\n",
      "total samples 347\n",
      "total samples 348\n",
      "total samples 349\n",
      "total samples 350\n",
      "total samples 351\n",
      "total samples 352\n",
      "total samples 353\n",
      "total samples 354\n",
      "total samples 355\n",
      "total samples 356\n",
      "total samples 357\n",
      "total samples 358\n",
      "total samples 359\n",
      "total samples 360\n",
      "total samples 361\n",
      "total samples 362\n",
      "total samples 363\n",
      "total samples 364\n",
      "total samples 365\n",
      "total samples 366\n",
      "total samples 367\n",
      "total samples 368\n",
      "total samples 369\n",
      "total samples 370\n",
      "total samples 371\n",
      "total samples 372\n",
      "total samples 373\n",
      "total samples 374\n",
      "total samples 375\n",
      "total samples 376\n",
      "total samples 377\n",
      "total samples 378\n",
      "total samples 379\n",
      "total samples 380\n",
      "total samples 381\n",
      "total samples 382\n",
      "total samples 383\n",
      "total samples 384\n",
      "total samples 385\n",
      "total samples 386\n",
      "total samples 387\n",
      "total samples 388\n",
      "total samples 389\n",
      "total samples 390\n",
      "total samples 391\n",
      "total samples 392\n",
      "total samples 393\n",
      "total samples 394\n",
      "total samples 395\n",
      "total samples 396\n",
      "total samples 397\n",
      "total samples 398\n",
      "total samples 399\n",
      "total samples 400\n",
      "total samples 401\n",
      "total samples 402\n",
      "total samples 403\n",
      "total samples 404\n",
      "total samples 405\n",
      "total samples 406\n",
      "total samples 407\n",
      "total samples 408\n",
      "total samples 409\n",
      "total samples 410\n",
      "total samples 411\n",
      "total samples 412\n",
      "total samples 413\n",
      "total samples 414\n",
      "total samples 415\n",
      "total samples 416\n",
      "total samples 417\n",
      "total samples 418\n",
      "total samples 419\n",
      "total samples 420\n",
      "total samples 421\n",
      "total samples 422\n",
      "total samples 423\n",
      "total samples 424\n",
      "total samples 425\n",
      "total samples 426\n",
      "total samples 427\n",
      "total samples 428\n",
      "total samples 429\n",
      "total samples 430\n",
      "total samples 431\n",
      "total samples 432\n",
      "total samples 433\n",
      "total samples 434\n",
      "total samples 435\n",
      "total samples 436\n",
      "total samples 437\n",
      "total samples 438\n",
      "total samples 439\n",
      "total samples 440\n",
      "total samples 441\n",
      "total samples 442\n",
      "total samples 443\n",
      "total samples 444\n",
      "total samples 445\n",
      "total samples 446\n",
      "total samples 447\n",
      "total samples 448\n",
      "total samples 449\n",
      "total samples 450\n",
      "total samples 451\n",
      "total samples 452\n",
      "total samples 453\n",
      "total samples 454\n",
      "total samples 455\n",
      "total samples 456\n",
      "total samples 457\n",
      "total samples 458\n",
      "total samples 459\n",
      "total samples 460\n",
      "total samples 461\n",
      "total samples 462\n",
      "total samples 463\n",
      "total samples 464\n",
      "total samples 465\n",
      "total samples 466\n",
      "total samples 467\n",
      "total samples 468\n",
      "total samples 469\n",
      "total samples 470\n",
      "total samples 471\n",
      "total samples 472\n",
      "total samples 473\n",
      "total samples 474\n",
      "total samples 475\n",
      "total samples 476\n",
      "total samples 477\n",
      "total samples 478\n",
      "total samples 479\n",
      "total samples 480\n",
      "total samples 481\n",
      "total samples 482\n",
      "total samples 483\n",
      "total samples 484\n",
      "total samples 485\n",
      "total samples 486\n",
      "total samples 487\n",
      "total samples 488\n",
      "total samples 489\n",
      "total samples 490\n",
      "total samples 491\n",
      "total samples 492\n",
      "total samples 493\n",
      "total samples 494\n",
      "total samples 495\n",
      "total samples 496\n",
      "total samples 497\n",
      "total samples 498\n",
      "total samples 499\n",
      "total samples 500\n",
      "total samples 501\n",
      "total samples 502\n",
      "total samples 503\n",
      "total samples 504\n",
      "total samples 505\n",
      "total samples 506\n",
      "total samples 507\n",
      "total samples 508\n",
      "total samples 509\n",
      "total samples 510\n",
      "total samples 511\n",
      "total samples 512\n",
      "total samples 513\n",
      "total samples 514\n",
      "total samples 515\n",
      "total samples 516\n",
      "total samples 517\n",
      "total samples 518\n",
      "total samples 519\n",
      "total samples 520\n",
      "total samples 521\n",
      "total samples 522\n",
      "total samples 523\n",
      "total samples 524\n",
      "total samples 525\n",
      "total samples 526\n",
      "total samples 527\n",
      "total samples 528\n",
      "total samples 529\n",
      "total samples 530\n",
      "total samples 531\n",
      "total samples 532\n",
      "total samples 533\n",
      "total samples 534\n",
      "total samples 535\n",
      "total samples 536\n",
      "total samples 537\n",
      "total samples 538\n",
      "total samples 539\n",
      "total samples 540\n",
      "total samples 541\n",
      "total samples 542\n",
      "total samples 543\n",
      "total samples 544\n",
      "total samples 545\n",
      "total samples 546\n",
      "total samples 547\n",
      "total samples 548\n",
      "total samples 549\n",
      "total samples 550\n",
      "total samples 551\n",
      "total samples 552\n",
      "total samples 553\n",
      "total samples 554\n",
      "total samples 555\n",
      "total samples 556\n",
      "total samples 557\n",
      "total samples 558\n",
      "total samples 559\n",
      "total samples 560\n",
      "total samples 561\n",
      "total samples 562\n",
      "total samples 563\n",
      "total samples 564\n",
      "total samples 565\n",
      "total samples 566\n",
      "total samples 567\n",
      "total samples 568\n",
      "total samples 569\n",
      "total samples 570\n",
      "total samples 571\n",
      "total samples 572\n",
      "total samples 573\n",
      "total samples 574\n",
      "total samples 575\n",
      "total samples 576\n",
      "total samples 577\n",
      "total samples 578\n",
      "total samples 579\n",
      "total samples 580\n",
      "total samples 581\n",
      "total samples 582\n",
      "total samples 583\n",
      "total samples 584\n",
      "total samples 585\n",
      "total samples 586\n",
      "total samples 587\n",
      "total samples 588\n",
      "total samples 589\n",
      "total samples 590\n",
      "total samples 591\n",
      "total samples 592\n",
      "total samples 593\n",
      "total samples 594\n",
      "total samples 595\n",
      "total samples 596\n",
      "total samples 597\n",
      "total samples 598\n",
      "total samples 599\n",
      "total samples 600\n",
      "total samples 601\n",
      "total samples 602\n",
      "total samples 603\n",
      "total samples 604\n",
      "total samples 605\n",
      "total samples 606\n",
      "total samples 607\n",
      "total samples 608\n",
      "total samples 609\n",
      "total samples 610\n",
      "total samples 611\n",
      "total samples 612\n",
      "total samples 613\n",
      "total samples 614\n",
      "total samples 615\n",
      "total samples 616\n",
      "total samples 617\n",
      "total samples 618\n",
      "total samples 619\n",
      "total samples 620\n",
      "total samples 621\n",
      "total samples 622\n",
      "total samples 623\n",
      "total samples 624\n",
      "total samples 625\n",
      "total samples 626\n",
      "total samples 627\n",
      "total samples 628\n",
      "total samples 629\n",
      "total samples 630\n",
      "total samples 631\n",
      "total samples 632\n",
      "total samples 633\n",
      "total samples 634\n",
      "total samples 635\n",
      "total samples 636\n",
      "total samples 637\n",
      "total samples 638\n",
      "total samples 639\n",
      "total samples 640\n",
      "total samples 641\n",
      "total samples 642\n",
      "total samples 643\n",
      "total samples 644\n",
      "total samples 645\n",
      "total samples 646\n",
      "total samples 647\n",
      "total samples 648\n",
      "total samples 649\n",
      "total samples 650\n",
      "total samples 651\n",
      "total samples 652\n",
      "total samples 653\n",
      "total samples 654\n",
      "total samples 655\n",
      "total samples 656\n",
      "total samples 657\n",
      "total samples 658\n",
      "total samples 659\n",
      "total samples 660\n",
      "total samples 661\n",
      "total samples 662\n",
      "total samples 663\n",
      "total samples 664\n",
      "total samples 665\n",
      "total samples 666\n",
      "total samples 667\n",
      "total samples 668\n",
      "total samples 669\n",
      "total samples 670\n",
      "total samples 671\n",
      "total samples 672\n",
      "total samples 673\n",
      "total samples 674\n",
      "total samples 675\n",
      "total samples 676\n",
      "total samples 677\n",
      "total samples 678\n",
      "total samples 679\n",
      "total samples 680\n",
      "total samples 681\n",
      "total samples 682\n",
      "total samples 683\n",
      "total samples 684\n",
      "total samples 685\n",
      "total samples 686\n",
      "total samples 687\n",
      "total samples 688\n",
      "total samples 689\n",
      "total samples 690\n",
      "total samples 691\n",
      "total samples 692\n",
      "total samples 693\n",
      "total samples 694\n",
      "total samples 695\n",
      "total samples 696\n",
      "total samples 697\n",
      "total samples 698\n",
      "total samples 699\n",
      "total samples 700\n",
      "total samples 701\n",
      "total samples 702\n",
      "total samples 703\n",
      "total samples 704\n",
      "total samples 705\n",
      "total samples 706\n",
      "total samples 707\n",
      "total samples 708\n",
      "total samples 709\n",
      "total samples 710\n",
      "total samples 711\n",
      "total samples 712\n",
      "total samples 713\n",
      "total samples 714\n",
      "total samples 715\n",
      "total samples 716\n",
      "total samples 717\n",
      "total samples 718\n",
      "total samples 719\n",
      "total samples 720\n",
      "total samples 721\n",
      "total samples 722\n",
      "total samples 723\n",
      "total samples 724\n",
      "total samples 725\n",
      "total samples 726\n",
      "total samples 727\n",
      "total samples 728\n",
      "total samples 729\n",
      "total samples 730\n",
      "total samples 731\n",
      "total samples 732\n",
      "total samples 733\n",
      "total samples 734\n",
      "total samples 735\n",
      "total samples 736\n",
      "total samples 737\n",
      "total samples 738\n",
      "total samples 739\n",
      "total samples 740\n",
      "total samples 741\n",
      "total samples 742\n",
      "total samples 743\n",
      "total samples 744\n",
      "total samples 745\n",
      "total samples 746\n",
      "total samples 747\n",
      "total samples 748\n",
      "total samples 749\n",
      "total samples 750\n",
      "total samples 751\n",
      "total samples 752\n",
      "total samples 753\n",
      "total samples 754\n",
      "total samples 755\n",
      "total samples 756\n",
      "total samples 757\n",
      "total samples 758\n",
      "total samples 759\n",
      "total samples 760\n",
      "total samples 761\n",
      "total samples 762\n",
      "total samples 763\n",
      "total samples 764\n",
      "total samples 765\n",
      "total samples 766\n",
      "total samples 767\n",
      "total samples 768\n",
      "total samples 769\n",
      "total samples 770\n",
      "total samples 771\n",
      "total samples 772\n",
      "total samples 773\n",
      "total samples 774\n",
      "total samples 775\n",
      "total samples 776\n",
      "total samples 777\n",
      "total samples 778\n",
      "total samples 779\n",
      "total samples 780\n",
      "total samples 781\n",
      "total samples 782\n",
      "total samples 783\n",
      "total samples 784\n",
      "total samples 785\n",
      "total samples 786\n",
      "total samples 787\n",
      "total samples 788\n",
      "total samples 789\n",
      "total samples 790\n",
      "total samples 791\n",
      "total samples 792\n",
      "total samples 793\n",
      "total samples 794\n",
      "total samples 795\n",
      "total samples 796\n",
      "total samples 797\n",
      "total samples 798\n",
      "total samples 799\n",
      "total samples 800\n",
      "total samples 801\n",
      "total samples 802\n",
      "total samples 803\n",
      "total samples 804\n",
      "total samples 805\n",
      "total samples 806\n",
      "total samples 807\n",
      "total samples 808\n",
      "total samples 809\n",
      "total samples 810\n",
      "total samples 811\n",
      "total samples 812\n",
      "total samples 813\n",
      "total samples 814\n",
      "total samples 815\n",
      "total samples 816\n",
      "total samples 817\n",
      "total samples 818\n",
      "total samples 819\n",
      "total samples 820\n",
      "total samples 821\n",
      "total samples 822\n",
      "total samples 823\n",
      "total samples 824\n",
      "total samples 825\n",
      "total samples 826\n",
      "total samples 827\n",
      "total samples 828\n",
      "total samples 829\n",
      "total samples 830\n",
      "total samples 831\n",
      "total samples 832\n",
      "total samples 833\n",
      "total samples 834\n",
      "total samples 835\n",
      "total samples 836\n",
      "total samples 837\n",
      "total samples 838\n",
      "total samples 839\n",
      "total samples 840\n",
      "total samples 841\n",
      "total samples 842\n",
      "total samples 843\n",
      "total samples 844\n",
      "total samples 845\n",
      "total samples 846\n",
      "total samples 847\n",
      "total samples 848\n",
      "total samples 849\n",
      "total samples 850\n",
      "total samples 851\n",
      "total samples 852\n",
      "total samples 853\n",
      "total samples 854\n",
      "total samples 855\n",
      "total samples 856\n",
      "total samples 857\n",
      "total samples 858\n",
      "total samples 859\n",
      "total samples 860\n",
      "total samples 861\n",
      "total samples 862\n",
      "total samples 863\n",
      "total samples 864\n",
      "total samples 865\n",
      "total samples 866\n",
      "total samples 867\n",
      "total samples 868\n",
      "total samples 869\n",
      "total samples 870\n",
      "total samples 871\n",
      "total samples 872\n",
      "total samples 873\n",
      "total samples 874\n",
      "total samples 875\n",
      "total samples 876\n",
      "total samples 877\n",
      "total samples 878\n",
      "total samples 879\n",
      "total samples 880\n",
      "total samples 881\n",
      "total samples 882\n",
      "total samples 883\n",
      "total samples 884\n",
      "total samples 885\n",
      "total samples 886\n",
      "total samples 887\n",
      "total samples 888\n",
      "total samples 889\n",
      "total samples 890\n",
      "total samples 891\n",
      "total samples 892\n",
      "total samples 893\n",
      "total samples 894\n",
      "total samples 895\n",
      "total samples 896\n",
      "total samples 897\n",
      "total samples 898\n",
      "total samples 899\n",
      "total samples 900\n",
      "total samples 901\n",
      "total samples 902\n",
      "total samples 903\n",
      "total samples 904\n",
      "total samples 905\n",
      "total samples 906\n",
      "total samples 907\n",
      "total samples 908\n",
      "total samples 909\n",
      "total samples 910\n",
      "total samples 911\n",
      "total samples 912\n",
      "total samples 913\n",
      "total samples 914\n",
      "total samples 915\n",
      "total samples 916\n",
      "total samples 917\n",
      "total samples 918\n",
      "total samples 919\n",
      "total samples 920\n",
      "total samples 921\n",
      "total samples 922\n",
      "total samples 923\n",
      "total samples 924\n",
      "total samples 925\n",
      "total samples 926\n",
      "total samples 927\n",
      "total samples 928\n",
      "total samples 929\n",
      "total samples 930\n",
      "total samples 931\n",
      "total samples 932\n",
      "total samples 933\n",
      "total samples 934\n",
      "total samples 935\n",
      "total samples 936\n",
      "total samples 937\n",
      "total samples 938\n",
      "total samples 939\n",
      "total samples 940\n",
      "total samples 941\n",
      "total samples 942\n",
      "total samples 943\n",
      "total samples 944\n",
      "total samples 945\n",
      "total samples 946\n",
      "total samples 947\n",
      "total samples 948\n",
      "total samples 949\n",
      "total samples 950\n",
      "total samples 951\n",
      "total samples 952\n",
      "total samples 953\n",
      "total samples 954\n",
      "total samples 955\n",
      "total samples 956\n",
      "total samples 957\n",
      "total samples 958\n",
      "total samples 959\n",
      "total samples 960\n",
      "total samples 961\n",
      "total samples 962\n",
      "total samples 963\n",
      "total samples 964\n",
      "total samples 965\n",
      "total samples 966\n",
      "total samples 967\n",
      "total samples 968\n",
      "total samples 969\n",
      "total samples 970\n",
      "total samples 971\n",
      "total samples 972\n",
      "total samples 973\n",
      "total samples 974\n",
      "total samples 975\n",
      "total samples 976\n",
      "total samples 977\n",
      "total samples 978\n",
      "total samples 979\n",
      "total samples 980\n",
      "total samples 981\n",
      "total samples 982\n",
      "total samples 983\n",
      "total samples 984\n",
      "total samples 985\n",
      "total samples 986\n",
      "total samples 987\n",
      "total samples 988\n",
      "total samples 989\n",
      "total samples 990\n",
      "total samples 991\n",
      "total samples 992\n",
      "total samples 993\n",
      "total samples 994\n",
      "total samples 995\n",
      "total samples 996\n",
      "total samples 997\n",
      "total samples 998\n",
      "total samples 999\n",
      "total samples 1000\n",
      "total samples 1001\n",
      "total samples 1002\n",
      "total samples 1003\n",
      "total samples 1004\n",
      "total samples 1005\n",
      "total samples 1006\n",
      "total samples 1007\n",
      "total samples 1008\n",
      "total samples 1009\n",
      "total samples 1010\n",
      "total samples 1011\n",
      "total samples 1012\n",
      "total samples 1013\n",
      "total samples 1014\n",
      "total samples 1015\n",
      "total samples 1016\n",
      "total samples 1017\n",
      "total samples 1018\n",
      "total samples 1019\n",
      "total samples 1020\n",
      "total samples 1021\n",
      "total samples 1022\n",
      "total samples 1023\n",
      "total samples 1024\n",
      "total samples 1025\n",
      "total samples 1026\n",
      "total samples 1027\n",
      "total samples 1028\n",
      "total samples 1029\n",
      "total samples 1030\n",
      "total samples 1031\n",
      "total samples 1032\n",
      "total samples 1033\n",
      "total samples 1034\n",
      "total samples 1035\n",
      "total samples 1036\n",
      "total samples 1037\n",
      "total samples 1038\n",
      "total samples 1039\n",
      "total samples 1040\n",
      "total samples 1041\n",
      "total samples 1042\n",
      "total samples 1043\n",
      "total samples 1044\n",
      "total samples 1045\n",
      "total samples 1046\n",
      "total samples 1047\n",
      "total samples 1048\n",
      "total samples 1049\n",
      "total samples 1050\n",
      "total samples 1051\n",
      "total samples 1052\n",
      "total samples 1053\n",
      "total samples 1054\n",
      "total samples 1055\n",
      "total samples 1056\n",
      "total samples 1057\n",
      "total samples 1058\n",
      "total samples 1059\n",
      "total samples 1060\n",
      "total samples 1061\n",
      "total samples 1062\n",
      "total samples 1063\n",
      "total samples 1064\n",
      "total samples 1065\n",
      "total samples 1066\n",
      "total samples 1067\n",
      "total samples 1068\n",
      "total samples 1069\n",
      "total samples 1070\n",
      "total samples 1071\n",
      "total samples 1072\n",
      "total samples 1073\n",
      "total samples 1074\n",
      "total samples 1075\n",
      "total samples 1076\n",
      "total samples 1077\n",
      "total samples 1078\n",
      "total samples 1079\n",
      "total samples 1080\n",
      "total samples 1081\n",
      "total samples 1082\n",
      "total samples 1083\n",
      "total samples 1084\n",
      "total samples 1085\n",
      "total samples 1086\n",
      "total samples 1087\n",
      "total samples 1088\n",
      "total samples 1089\n",
      "total samples 1090\n",
      "total samples 1091\n",
      "total samples 1092\n",
      "total samples 1093\n",
      "total samples 1094\n",
      "total samples 1095\n",
      "total samples 1096\n",
      "total samples 1097\n",
      "total samples 1098\n",
      "total samples 1099\n",
      "total samples 1100\n",
      "total samples 1101\n",
      "total samples 1102\n",
      "total samples 1103\n",
      "total samples 1104\n",
      "total samples 1105\n",
      "total samples 1106\n",
      "total samples 1107\n",
      "total samples 1108\n",
      "total samples 1109\n",
      "total samples 1110\n",
      "total samples 1111\n",
      "total samples 1112\n",
      "total samples 1113\n",
      "total samples 1114\n",
      "total samples 1115\n",
      "total samples 1116\n",
      "total samples 1117\n",
      "total samples 1118\n",
      "total samples 1119\n",
      "total samples 1120\n",
      "total samples 1121\n",
      "total samples 1122\n",
      "total samples 1123\n",
      "total samples 1124\n",
      "total samples 1125\n",
      "total samples 1126\n",
      "total samples 1127\n",
      "total samples 1128\n",
      "total samples 1129\n",
      "total samples 1130\n",
      "total samples 1131\n",
      "total samples 1132\n",
      "total samples 1133\n",
      "total samples 1134\n",
      "total samples 1135\n",
      "total samples 1136\n",
      "total samples 1137\n",
      "total samples 1138\n",
      "total samples 1139\n",
      "total samples 1140\n",
      "total samples 1141\n",
      "total samples 1142\n",
      "total samples 1143\n",
      "total samples 1144\n",
      "total samples 1145\n",
      "total samples 1146\n",
      "total samples 1147\n",
      "total samples 1148\n",
      "total samples 1149\n",
      "total samples 1150\n",
      "total samples 1151\n",
      "total samples 1152\n",
      "total samples 1153\n",
      "total samples 1154\n",
      "total samples 1155\n",
      "total samples 1156\n",
      "total samples 1157\n",
      "total samples 1158\n",
      "total samples 1159\n",
      "total samples 1160\n",
      "total samples 1161\n",
      "total samples 1162\n",
      "total samples 1163\n",
      "total samples 1164\n",
      "total samples 1165\n",
      "total samples 1166\n",
      "total samples 1167\n",
      "total samples 1168\n",
      "total samples 1169\n",
      "total samples 1170\n",
      "total samples 1171\n",
      "total samples 1172\n",
      "total samples 1173\n",
      "total samples 1174\n",
      "total samples 1175\n",
      "total samples 1176\n",
      "total samples 1177\n",
      "total samples 1178\n",
      "total samples 1179\n",
      "total samples 1180\n",
      "total samples 1181\n",
      "total samples 1182\n",
      "total samples 1183\n",
      "total samples 1184\n",
      "total samples 1185\n",
      "total samples 1186\n",
      "total samples 1187\n",
      "total samples 1188\n",
      "total samples 1189\n",
      "total samples 1190\n",
      "total samples 1191\n",
      "total samples 1192\n",
      "total samples 1193\n",
      "total samples 1194\n",
      "total samples 1195\n",
      "total samples 1196\n",
      "total samples 1197\n",
      "total samples 1198\n",
      "total samples 1199\n",
      "total samples 1200\n",
      "total samples 1201\n",
      "total samples 1202\n",
      "total samples 1203\n",
      "total samples 1204\n",
      "total samples 1205\n",
      "total samples 1206\n",
      "total samples 1207\n",
      "total samples 1208\n",
      "total samples 1209\n",
      "total samples 1210\n",
      "total samples 1211\n",
      "total samples 1212\n",
      "total samples 1213\n",
      "total samples 1214\n",
      "total samples 1215\n",
      "total samples 1216\n",
      "total samples 1217\n",
      "total samples 1218\n",
      "total samples 1219\n",
      "total samples 1220\n",
      "total samples 1221\n",
      "total samples 1222\n",
      "total samples 1223\n",
      "total samples 1224\n",
      "total samples 1225\n",
      "total samples 1226\n",
      "total samples 1227\n",
      "total samples 1228\n",
      "total samples 1229\n",
      "total samples 1230\n",
      "total samples 1231\n",
      "total samples 1232\n",
      "total samples 1233\n",
      "total samples 1234\n",
      "total samples 1235\n",
      "total samples 1236\n",
      "total samples 1237\n",
      "total samples 1238\n",
      "total samples 1239\n",
      "total samples 1240\n",
      "total samples 1241\n",
      "total samples 1242\n",
      "total samples 1243\n",
      "total samples 1244\n",
      "total samples 1245\n",
      "total samples 1246\n",
      "total samples 1247\n",
      "total samples 1248\n",
      "total samples 1249\n",
      "total samples 1250\n",
      "total samples 1251\n",
      "total samples 1252\n",
      "total samples 1253\n",
      "total samples 1254\n",
      "total samples 1255\n",
      "total samples 1256\n",
      "total samples 1257\n",
      "total samples 1258\n",
      "total samples 1259\n",
      "total samples 1260\n",
      "total samples 1261\n",
      "total samples 1262\n",
      "total samples 1263\n",
      "total samples 1264\n",
      "total samples 1265\n",
      "total samples 1266\n",
      "total samples 1267\n",
      "total samples 1268\n",
      "total samples 1269\n",
      "total samples 1270\n",
      "total samples 1271\n",
      "total samples 1272\n",
      "total samples 1273\n",
      "total samples 1274\n",
      "total samples 1275\n",
      "total samples 1276\n",
      "total samples 1277\n",
      "total samples 1278\n",
      "total samples 1279\n",
      "total samples 1280\n",
      "total samples 1281\n",
      "total samples 1282\n",
      "total samples 1283\n",
      "total samples 1284\n",
      "total samples 1285\n",
      "total samples 1286\n",
      "total samples 1287\n",
      "total samples 1288\n",
      "total samples 1289\n",
      "total samples 1290\n",
      "total samples 1291\n",
      "total samples 1292\n",
      "total samples 1293\n",
      "total samples 1294\n",
      "total samples 1295\n",
      "total samples 1296\n",
      "total samples 1297\n",
      "total samples 1298\n",
      "total samples 1299\n",
      "total samples 1300\n",
      "total samples 1301\n",
      "total samples 1302\n",
      "total samples 1303\n",
      "total samples 1304\n",
      "total samples 1305\n",
      "total samples 1306\n",
      "total samples 1307\n",
      "total samples 1308\n",
      "total samples 1309\n",
      "total samples 1310\n",
      "total samples 1311\n",
      "total samples 1312\n",
      "total samples 1313\n",
      "total samples 1314\n",
      "total samples 1315\n",
      "total samples 1316\n",
      "total samples 1317\n",
      "total samples 1318\n",
      "total samples 1319\n",
      "total samples 1320\n",
      "total samples 1321\n",
      "total samples 1322\n",
      "total samples 1323\n",
      "total samples 1324\n",
      "total samples 1325\n",
      "total samples 1326\n",
      "total samples 1327\n",
      "total samples 1328\n",
      "total samples 1329\n",
      "total samples 1330\n",
      "total samples 1331\n",
      "total samples 1332\n",
      "total samples 1333\n",
      "total samples 1334\n",
      "total samples 1335\n",
      "total samples 1336\n",
      "total samples 1337\n",
      "total samples 1338\n",
      "total samples 1339\n",
      "total samples 1340\n",
      "total samples 1341\n",
      "total samples 1342\n",
      "total samples 1343\n",
      "total samples 1344\n",
      "total samples 1345\n",
      "total samples 1346\n",
      "total samples 1347\n",
      "total samples 1348\n",
      "total samples 1349\n",
      "total samples 1350\n",
      "Filtered 1350 images and 1350 labels.\n"
     ]
    }
   ],
   "source": [
    "new_image_filenames = []\n",
    "new_label_filenames = []\n",
    "\n",
    "total_samples = 0\n",
    "break_flag = 0\n",
    "\n",
    "for subfolder in os.listdir(base_dir):\n",
    "    subfolder_path = os.path.join(base_dir, subfolder)\n",
    "    \n",
    "    if os.path.isdir(subfolder_path):\n",
    "        # Find image files with '-t2f.nii.gz'\n",
    "        image_files = glob.glob(subfolder_path + '/*-t2f.nii.gz')\n",
    "        # Find corresponding label files with '-seg.nii.gz'\n",
    "        label_files = glob.glob(subfolder_path + '/*-seg.nii.gz')\n",
    "\n",
    "        for image_file, label_file in zip(image_files, label_files):\n",
    "\n",
    "            new_image_filenames.append(image_file)\n",
    "            new_label_filenames.append(label_file)\n",
    "        \n",
    "            total_samples += 1\n",
    "            print(f'total samples {total_samples}')\n",
    "            \n",
    "#             if total_samples == 10:\n",
    "#                 break_flag = 1\n",
    "                \n",
    "            if break_flag:\n",
    "                break\n",
    "                \n",
    "        if break_flag:\n",
    "            break\n",
    "\n",
    "print(f\"Filtered {len(new_image_filenames)} images and {len(new_label_filenames)} labels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BraTSDatasetII(Dataset):\n",
    "\n",
    "    def __init__(self, new_image_filenames,new_label_filenames):\n",
    "        self.image_names = new_image_filenames\n",
    "        self.label_mask_names = new_label_filenames\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "    \n",
    "    \n",
    "    def convert_nonzero_to_one(self, mask_tensor):\n",
    "        for i in range(mask_tensor.shape[0]):\n",
    "            for j in range(mask_tensor.shape[1]):\n",
    "                for k in range(mask_tensor.shape[2]):\n",
    "                    if int(mask_tensor[i, j, k]) > 1:\n",
    "                        mask_tensor[i, j, k] = 1.0\n",
    "                        \n",
    "        return mask_tensor\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # Reading NIfTI image file\n",
    "        image = sitk.ReadImage(self.image_names[idx])\n",
    "\n",
    "        # Reading NIfTI label file\n",
    "        label_mask = sitk.ReadImage(self.label_mask_names[idx])\n",
    "\n",
    "        # Desired output size\n",
    "        desired_size = (128, 128, 128)  # Target shape\n",
    "\n",
    "        # Get original size\n",
    "        original_size = image.GetSize()  # Original size in voxels (x, y, z)\n",
    "\n",
    "        # Calculate the new spacing based on desired size\n",
    "        original_spacing = image.GetSpacing()  # Current spacing in mm\n",
    "        new_spacing = [\n",
    "            original_spacing[i] * (original_size[i] / desired_size[i]) for i in range(3)\n",
    "        ]\n",
    "\n",
    "        # Resize the image to the desired size\n",
    "        resampled_img = sitk.Resample(\n",
    "            image,\n",
    "            desired_size,\n",
    "            sitk.Transform(),\n",
    "            sitk.sitkLinear,  # Linear interpolation for images\n",
    "            image.GetOrigin(),\n",
    "            new_spacing,\n",
    "            image.GetDirection(),\n",
    "            0,\n",
    "            image.GetPixelID()\n",
    "        )\n",
    "\n",
    "        # Resize the mask to the desired size\n",
    "        resampled_mask = sitk.Resample(\n",
    "            label_mask,\n",
    "            desired_size,\n",
    "            sitk.Transform(),\n",
    "            sitk.sitkNearestNeighbor,  # Nearest-neighbor for label masks\n",
    "            label_mask.GetOrigin(),\n",
    "            new_spacing,\n",
    "            label_mask.GetDirection(),\n",
    "            0,\n",
    "            label_mask.GetPixelID()\n",
    "        )\n",
    "\n",
    "        # Convert to NumPy arrays\n",
    "        image_data = sitk.GetArrayFromImage(resampled_img)\n",
    "        label_mask_data = sitk.GetArrayFromImage(resampled_mask)\n",
    "\n",
    "        # Convert NumPy arrays to torch tensors\n",
    "        image_tensor = torch.from_numpy(image_data).unsqueeze(0).float()  # Add channel dimension\n",
    "        label_tensor = torch.from_numpy(label_mask_data.astype(np.float32))\n",
    "\n",
    "        # Convert all non-zero values in the mask to 1\n",
    "        label_tensor = self.convert_nonzero_to_one(label_tensor)\n",
    "\n",
    "        # Normalize the image tensor\n",
    "        image_tensor = (image_tensor - torch.mean(image_tensor)) / torch.std(image_tensor)\n",
    "\n",
    "        return image_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BraTSDatasetII(new_image_filenames,new_label_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.9\n",
    "train_split = int(train_ratio * len(dataset))\n",
    "test_split = int(len(dataset) - train_split)\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_split, test_split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory = True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True, pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/scratch/pbs.7011.srv-svkmmastermum.x8z/wandb/run-20240928_110858-rva4gxlj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/brats_2024_3d_unet/brats%203d%20unet/runs/rva4gxlj' target=\"_blank\">rare-butterfly-59</a></strong> to <a href='https://wandb.ai/brats_2024_3d_unet/brats%203d%20unet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/brats_2024_3d_unet/brats%203d%20unet' target=\"_blank\">https://wandb.ai/brats_2024_3d_unet/brats%203d%20unet</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/brats_2024_3d_unet/brats%203d%20unet/runs/rva4gxlj' target=\"_blank\">https://wandb.ai/brats_2024_3d_unet/brats%203d%20unet/runs/rva4gxlj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/brats_2024_3d_unet/brats%203d%20unet/runs/rva4gxlj?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7ff8dae77220>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"brats 3d unet\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "dgb3JkrCOlkm"
   },
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet3DD(nn.Module):\n",
    "    def __init__(self, verbose):\n",
    "        super(UNet3DD, self).__init__()\n",
    "        \n",
    "        self.verbose = verbose\n",
    "        \n",
    "        def conv_block(in_channels, out_channels):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm3d(out_channels),\n",
    "                nn.LeakyReLU(negative_slope=0.01),\n",
    "                nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm3d(out_channels),\n",
    "                nn.LeakyReLU(negative_slope=0.01)\n",
    "            )\n",
    "        \n",
    "        def up_conv_block(in_channels, out_channels):\n",
    "            return nn.ConvTranspose3d(in_channels, out_channels, kernel_size=2, stride=3, padding=1)\n",
    "        \n",
    "        # Downward path (encoding)\n",
    "        self.conv_downwards = nn.ModuleList([\n",
    "            conv_block(1, 64),  # Conv block 1\n",
    "            nn.MaxPool3d(kernel_size=2, stride=2),  # MaxPool after block 1\n",
    "            conv_block(64, 128),  # Conv block 2\n",
    "            nn.MaxPool3d(kernel_size=2, stride=2),  # MaxPool after block 2\n",
    "            conv_block(128, 256),  # Conv block 3\n",
    "            nn.MaxPool3d(kernel_size=2, stride=2),  # MaxPool after block 3\n",
    "            conv_block(256, 512),  # Conv block 4\n",
    "            nn.MaxPool3d(kernel_size=2, stride=2)   # MaxPool after block 4\n",
    "        ])\n",
    "        \n",
    "        # Bottleneck layer\n",
    "        self.bottleneck_layer = nn.Sequential(\n",
    "            nn.Conv3d(512, 1024, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(1024),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.Conv3d(1024, 1024, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(1024),\n",
    "            nn.LeakyReLU(negative_slope=0.01)\n",
    "        )\n",
    "        \n",
    "        # Upward path (decoding)\n",
    "        self.conv_upwards = nn.ModuleList([\n",
    "            up_conv_block(1024, 512),  # UpConv block 1\n",
    "            conv_block(1024, 512),  # Conv block after UpConv 1\n",
    "            up_conv_block(512, 256),  # UpConv block 2\n",
    "            conv_block(512, 256),  # Conv block after UpConv 2\n",
    "            up_conv_block(256, 128),  # UpConv block 3\n",
    "            conv_block(256, 128),  # Conv block after UpConv 3\n",
    "            up_conv_block(128, 64),  # UpConv block 4\n",
    "            conv_block(128, 64)  # Conv block after UpConv 4\n",
    "        ])\n",
    "        \n",
    "        # Final 1x1 Conv layer\n",
    "        self.conv_1x1 = nn.Conv3d(64, 1, kernel_size=1, padding=1)\n",
    "        \n",
    "        # Sigmoid\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def center_crop(self, tensor, target_dims):\n",
    "\n",
    "        current_dims= tensor.shape\n",
    "        start_indices = [(curr_dim - target_dim) // 2 for curr_dim, target_dim in zip(current_dims, target_dims)]\n",
    "        end_indices = [start + target_dim for start, target_dim in zip(start_indices, target_dims)]\n",
    "\n",
    "        cropped_data = tensor[\n",
    "            :,\n",
    "            :, \n",
    "            start_indices[2]:end_indices[2],\n",
    "            start_indices[3]:end_indices[3],\n",
    "            start_indices[4]:end_indices[4],\n",
    "\n",
    "        ]\n",
    "\n",
    "        return cropped_data\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        original_x_shape = x.shape\n",
    "        # Downward path\n",
    "        skip_connections = []\n",
    "        for layer in self.conv_downwards:\n",
    "            if isinstance(layer, nn.MaxPool3d):\n",
    "                skip_connections.append(x)  # Save skip connection before MaxPool\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f'applying {layer} => {x.shape}\\n\\n')\n",
    "            x = layer(x)\n",
    "\n",
    "        # Bottleneck\n",
    "        x = self.bottleneck_layer(x)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f'applying bottleneck layer {self.bottleneck_layer} => {x.shape}\\n\\n')\n",
    "        \n",
    "        # Upward path\n",
    "        for i in range(0, len(self.conv_upwards), 2):\n",
    "            x = self.conv_upwards[i](x)  # UpConvolution\n",
    "            if self.verbose:\n",
    "                print(f'applying {self.conv_upwards[i]} => {x.shape}')\n",
    "              \n",
    "            popped_x = skip_connections.pop()\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f'CONCATENATING')\n",
    "                print('popped_x shape is ', popped_x.shape)\n",
    "                print(f'x shape before concatenating is {x.shape}\\n\\n')\n",
    "            x = self.center_crop(x, popped_x.shape)\n",
    "            x = torch.cat((popped_x, x), dim=1)  # Concatenate with skip connection\n",
    "            x = self.conv_upwards[i + 1](x)  # Conv block after concatenation\n",
    "            if self.verbose:\n",
    "                print(f'applying layer {self.conv_upwards[i + 1]} => {x.shape}\\n\\n')\n",
    "\n",
    "        # Final 1x1 Conv\n",
    "        x = self.conv_1x1(x)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f'applying conv 1x1 => {x.shape}\\n\\n')\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        x = self.center_crop(x, original_x_shape)\n",
    "        \n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DICE LOSS ####\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "    \n",
    "    def dice_coeff(self, pred, target, smooth=1e-6):\n",
    "        \"\"\"\n",
    "        Compute the Dice coefficient for binary classification.\n",
    "\n",
    "        Parameters:\n",
    "        - pred: Predicted tensor (probabilities).\n",
    "        - target: Ground truth tensor (binary).\n",
    "        - smooth: Smoothing factor to avoid division by zero.\n",
    "\n",
    "        Returns:\n",
    "        - Dice coefficient (mean over batch).\n",
    "        \"\"\"\n",
    "        # Flatten tensors\n",
    "        pred = pred.view(-1)\n",
    "        target = target.view(-1)\n",
    "\n",
    "        # Compute intersection and union\n",
    "        intersection = (pred * target).sum()\n",
    "        union = pred.sum() + target.sum()\n",
    "\n",
    "        # Compute Dice coefficient\n",
    "        dice = (2. * intersection + smooth) / (union + smooth)\n",
    "        return dice\n",
    "\n",
    "    def dice_loss(self, pred, target):\n",
    "        \"\"\"\n",
    "        Compute the Dice loss for binary classification.\n",
    "\n",
    "        Parameters:\n",
    "        - pred: Predicted tensor (logits or probabilities).\n",
    "        - target: Ground truth tensor (binary).\n",
    "        - smooth: Smoothing factor to avoid division by zero.\n",
    "\n",
    "        Returns:\n",
    "        - Dice loss (1 - Dice coefficient).\n",
    "        \"\"\"\n",
    "        # Apply sigmoid to get probabilities if not already applied\n",
    "        pred = torch.sigmoid(pred)\n",
    "\n",
    "        # Compute Dice coefficient\n",
    "        dice = self.dice_coeff(pred, target, smooth=self.smooth)\n",
    "\n",
    "        # Dice loss is 1 - Dice coefficient\n",
    "        return 1 - dice\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        return self.dice_loss(pred, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Tversky  LOSS ####\n",
    "\n",
    "class TverskyLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, alpha=0.7, beta=0.3, smooth=1e-6):\n",
    "        super(TverskyLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.smooth = smooth\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        \"\"\"\n",
    "        Compute the Tversky loss for binary classification.\n",
    "\n",
    "        Parameters:\n",
    "        - pred: Predicted tensor (logits or probabilities).\n",
    "        - target: Ground truth tensor (binary).\n",
    "\n",
    "        Returns:\n",
    "        - Tversky loss.\n",
    "        \"\"\"\n",
    "\n",
    "        # Flatten tensors\n",
    "        pred = pred.reshape(-1)\n",
    "        target = target.reshape(-1)\n",
    "\n",
    "        # Compute true positives, false positives, and false negatives\n",
    "        true_positive = (pred * target).sum()\n",
    "        false_positive = ((1 - target) * pred).sum()\n",
    "        false_negative = (target * (1 - pred)).sum()\n",
    "\n",
    "        # Compute Tversky coefficient\n",
    "        tversky_coeff = (true_positive + self.smooth) / (true_positive + self.alpha * false_positive + self.beta * false_negative + self.smooth)\n",
    "        \n",
    "        # Tversky loss is 1 - Tversky coefficient\n",
    "        return 1 - tversky_coeff\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unet3d = UNet3DD(verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_loss_fn = DiceLoss(smooth=1e-6)\n",
    "tversky_loss_fn = TverskyLoss(alpha=0.4, beta=0.6, smooth=1e-6)\n",
    "bce_loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data, masks = next(iter(train_dataloader))\n",
    "# data, masks = data.to(device), masks.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = unet3d(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unet3d(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.Trial):\n",
    "    lr = trial.suggest_float(\"lr\", 0.05, 0.1)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 1e-4)\n",
    "    beta1 = trial.suggest_float(\"beta1\", 0.7, 0.99)\n",
    "    beta2 = trial.suggest_float(\"beta2\", 0.95, 0.99)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(unet3d.parameters(), lr=lr, weight_decay=weight_decay, betas=(beta1, beta2))\n",
    "    \n",
    "    data, masks = next(iter(train_dataloader))\n",
    "    data, masks = data.to(device), masks.to(device)\n",
    "    \n",
    "    loss = loss_fn(unet3d(data), masks)\n",
    "    \n",
    "    del data\n",
    "    del masks\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study = optuna.create_study()\n",
    "# study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.Adam(unet3d.parameters(), lr=study.best_params['lr'], weight_decay=study.best_params['weight_decay'], betas=(study.best_params['beta1'], study.best_params['beta2']))\n",
    "optimizer = torch.optim.Adam(unet3d.parameters(), lr=3e-4, betas=(0.9, 0.999), weight_decay = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C9F9WuwIP7ck",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "checkpoint_identifier = f'20240928_' + str(time.strftime(\"%H:%M\"))\n",
    "\n",
    "# Create a directory to save checkpoints\n",
    "checkpoint_dir = \"/data/mpstme-priyanka/\" + checkpoint_identifier + \"_tversky_checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch_idx, (data, masks) in enumerate(train_dataloader):\n",
    "\n",
    "        data = data.to(device)\n",
    "        masks = masks.long().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        pred = unet3d(data)\n",
    "        target = masks\n",
    "        \n",
    "        train_loss = dice_loss_fn(pred, target) + bce_loss_fn(pred.squeeze(1), target.float()) + tversky_loss_fn(pred, target)\n",
    "\n",
    "        print(f'Batch {batch_idx + 1} : Loss {train_loss}')\n",
    "        wandb.log({\"train_loss\": train_loss.item()})\n",
    "        \n",
    "        if batch_idx % 5 == 0:\n",
    "            \n",
    "            # Getting test data and masks\n",
    "            test_data, test_masks = next(iter(test_dataloader))\n",
    "            test_data, test_masks = test_data.to(device), test_masks.to(device)\n",
    "\n",
    "            test_pred = unet3d(test_data)\n",
    "            test_loss = loss_fn(test_pred, test_masks)\n",
    "\n",
    "            wandb.log({\"test_loss\": test_loss.item()})\n",
    "\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "            # Train Prediction\n",
    "            axes[0, 0].imshow(pred[0][0][64].cpu().detach().numpy(), cmap='gray')\n",
    "            axes[0, 0].set_title(\"Train Prediction\")\n",
    "            axes[0, 0].axis('off')\n",
    "\n",
    "            # Train Target\n",
    "            axes[0, 1].imshow(target[0][64].cpu().detach().numpy(), cmap='gray')\n",
    "            axes[0, 1].set_title(\"Train Target\")\n",
    "            axes[0, 1].axis('off')\n",
    "            \n",
    "            axes[1, 0].imshow(test_pred[0][0][64].cpu().detach().numpy(), cmap='gray')\n",
    "            axes[1, 0].set_title(\"Test Prediction\")\n",
    "            axes[1, 0].axis('off')\n",
    "\n",
    "            # Train Target\n",
    "            axes[1, 1].imshow(test_masks[0][64].cpu().detach().numpy(), cmap='gray')\n",
    "            axes[1, 1].set_title(\"Test Target\")\n",
    "            axes[1, 1].axis('off')\n",
    "\n",
    "            # Adjust layout\n",
    "            plt.tight_layout()\n",
    "            wandb.log({\"Predictions\": wandb.Image(fig)})\n",
    "            plt.close(fig)\n",
    "\n",
    "        epoch_loss += train_loss.item()\n",
    "        \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Save a checkpoint after each epoch\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f'{checkpoint_identifier}_checkpoint_epoch_{epoch + 1}.pth')\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': unet3d.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss': epoch_loss,\n",
    "    }, checkpoint_path)\n",
    "    \n",
    "    print(f'EPOCH {epoch + 1} ; LOSS {epoch_loss}\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = unet3d(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask_file = sitk.GetImageFromArray(target[0].cpu().detach().numpy())\n",
    "# mask_file = sitk.Cast(mask_file, sitk.sitkUInt8)\n",
    "\n",
    "# pred_file = sitk.GetImageFromArray(output[0][0].cpu().detach().numpy())\n",
    "\n",
    "# sitk.WriteImage(mask_file, \"masks_20240923.nii.gz\")\n",
    "# sitk.WriteImage(pred_file, \"pred_20240923.nii.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(unet3d.state_dict(), 'brats_3d_unet_epochs_36_20240928.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_dir = base_dir = '/data/mpstme-priyanka/BraTS2024-BraTS-GLI-ValidationData/validation_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_image_filenames = []\n",
    "# total_samples = 0\n",
    "# break_flag = 0\n",
    "\n",
    "# for subfolder in os.listdir(val_dir):\n",
    "#     subfolder_path = os.path.join(val_dir, subfolder)\n",
    "    \n",
    "#     if os.path.isdir(subfolder_path):\n",
    "#         # Find image files with '-t2f.nii.gz'\n",
    "#         image_files = glob.glob(subfolder_path + '/*-t2f.nii.gz')\n",
    "\n",
    "#         for image_file in image_files:  # Iterate directly over image_files\n",
    "#             image = sitk.ReadImage(image_file)\n",
    "            \n",
    "#             # Cast the image to float32\n",
    "#             image = sitk.Cast(image, sitk.sitkFloat32)\n",
    "            \n",
    "#             # Convert to a NumPy array\n",
    "#             image_data = sitk.GetArrayFromImage(image)\n",
    "#             image_data_shape = image_data.shape\n",
    "\n",
    "#             # Optionally, check size conditions if needed\n",
    "#             # if image_data_shape[0] >= 182 and image_data_shape[1] >= 218 and image_data_shape[2] >= 182:\n",
    "#             new_image_filenames.append(image_file)\n",
    "           \n",
    "#             total_samples += 1\n",
    "#             print(f'Total samples: {total_samples}')\n",
    "            \n",
    "#             if total_samples == 4:\n",
    "#                 break_flag = 1\n",
    "                \n",
    "#             if break_flag:\n",
    "#                 break\n",
    "                \n",
    "#         if break_flag:\n",
    "#             break\n",
    "\n",
    "# print(f\"Filtered {len(new_image_filenames)} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BraTSDataset_val(Dataset):\n",
    "\n",
    "#     def __init__(self, new_image_filenames):\n",
    "#         self.image_names = new_image_filenames\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.image_names)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         # Load the NIfTI file\n",
    "#         image = sitk.ReadImage(self.image_names[idx])\n",
    "        \n",
    "#         # Downsample the image\n",
    "#         new_size = [int(dim / 2) for dim in image.GetSize()]\n",
    "#         downsampled_img = sitk.Resample(\n",
    "#             image,\n",
    "#             new_size,\n",
    "#             sitk.Transform(),\n",
    "#             sitk.sitkLinear,\n",
    "#             image.GetOrigin(),\n",
    "#             [sz * 2 for sz in image.GetSpacing()],\n",
    "#             image.GetDirection(),\n",
    "#             0,\n",
    "#             image.GetPixelID()\n",
    "#         )\n",
    "\n",
    "#         # Convert to a NumPy array\n",
    "#         image_data = sitk.GetArrayFromImage(downsampled_img)\n",
    "\n",
    "#         # Reshape data to add channel (1) and convert to torch tensor\n",
    "#         image_tensor = torch.from_numpy(image_data).unsqueeze(0).float()\n",
    "        \n",
    "#         # Normalize the image tensor\n",
    "#         image_tensor = (image_tensor - torch.mean(image_tensor)) / torch.std(image_tensor)\n",
    "\n",
    "#         return image_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_dataset = BraTSDataset_val(new_image_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_dataloader =  DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_data = next(iter(val_dataloader))\n",
    "# val_data = val_data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_out = unet3d(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # mask_file = sitk.GetImageFromArray(target[0].cpu().detach().numpy())\n",
    "# # mask_file = sitk.Cast(mask_file, sitk.sitkUInt8)\n",
    "\n",
    "# mask_new_shape = sitk.GetImageFromArray(masks[0].cpu().detach().numpy())\n",
    "\n",
    "# # sitk.WriteImage(mask_file, \"masks_20240923.nii.gz\")\n",
    "# sitk.WriteImage(mask_new_shape, \"mask_new_shape_20240928.nii.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Sep 28 11:16:53 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.86.10              Driver Version: 535.86.10    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA H100 PCIe               Off | 00000000:CA:00.0 Off |                   On |\n",
      "| N/A   29C    P0              52W / 350W |                  N/A |     N/A      Default |\n",
      "|                                         |                      |              Enabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "\n",
      "+---------------------------------------------------------------------------------------+\n",
      "| MIG devices:                                                                          |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "| GPU  GI  CI  MIG |                   Memory-Usage |        Vol|      Shared           |\n",
      "|      ID  ID  Dev |                     BAR1-Usage | SM     Unc| CE ENC DEC OFA JPG    |\n",
      "|                  |                                |        ECC|                       |\n",
      "|==================+================================+===========+=======================|\n",
      "|  0    1   0   0  |              41MiB / 40320MiB  | 46      0 |  3   0    3    0    3 |\n",
      "|                  |               1MiB / 65535MiB  |           |                       |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "|  0    2   0   1  |              37MiB / 40320MiB  | 46      0 |  3   0    3    0    3 |\n",
      "|                  |               0MiB / 65535MiB  |           |                       |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
